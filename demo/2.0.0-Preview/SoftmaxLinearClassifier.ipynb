{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "In this article, we will use [softmax](https://en.wikipedia.org/wiki/Softmax_function) classifier to build a simple image classification neural network with an accuracy of 32%. In a Softmax classifier, binary logic is generalized and regressed to multiple logic. Softmax classifier will output the probability of the corresponding category.\n",
    "\n",
    "We will first define a softmax classifier, then use the training set of [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) to train the neural network, and finally use the test set to verify the accuracy of the neural network.\n",
    "\n",
    "Let’s get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous course [GettingStarted](https://thoughtworksinc.github.io/DeepLearning.scala/demo/GettingStarted.html), we need to introduce each class of DeepLearning.scala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                             \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$plugin.$                                            \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.ExecutionContext.Implicits.global\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.api.ndarray.INDArray\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.nd4j.linalg.factory.Nd4j\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.deeplearning.plugins.Builtins\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.feature.Factory\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.element._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.layout._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mplotly.JupyterScala._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.future._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.Await\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent.duration.Duration\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.thoughtworks.each.Monadic._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscalaz.std.stream._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.nd4j::nd4s:0.8.0`\n",
    "import $ivy.`org.nd4j:nd4j-native-platform:0.8.0`\n",
    "import $ivy.`com.chuusai::shapeless:2.3.2`\n",
    "import $ivy.`org.rauschig:jarchivelib:0.5.0`\n",
    "import $ivy.`com.thoughtworks.deeplearning::plugins-builtins:2.0.0-RC6`\n",
    "import $ivy.`org.plotly-scala::plotly-jupyter-scala:0.3.2`\n",
    "import $ivy.`com.thoughtworks.each::each:3.3.1`\n",
    "import $plugin.$ivy.`org.scalamacros:paradise_2.11.11:2.1.0`\n",
    "\n",
    "import scala.concurrent.ExecutionContext.Implicits.global\n",
    "import org.nd4j.linalg.api.ndarray.INDArray\n",
    "import org.nd4j.linalg.factory.Nd4j\n",
    "import com.thoughtworks.deeplearning.plugins.Builtins\n",
    "import com.thoughtworks.feature.Factory\n",
    "import plotly._\n",
    "import plotly.element._\n",
    "import plotly.layout._\n",
    "import plotly.JupyterScala._\n",
    "import com.thoughtworks.future._\n",
    "import scala.concurrent.Await\n",
    "import scala.concurrent.duration.Duration\n",
    "import com.thoughtworks.each.Monadic._\n",
    "import scalaz.std.stream._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the line numbers outputted by `jupyter-scala` and to make sure that the page output will not be too long, we need to set `pprintConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pprintConfig() = pprintConfig().copy(height = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your own neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning rate need to be set for the full connection layer. Learning rate visually describes the change rate of `weight`. A too-low learning rate will result in slow decrease of `loss`, which will require longer time for training; A too-high learning rate will result in rapid decrease of `loss` at first while fluctuation around the lowest point afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mINDArrayLearningRatePluginUrl\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"https://gist.githubusercontent.com/TerrorJack/118487016d7973d67feb489449dee156/raw/778bb1b68a664c752b0945111220326731310214/INDArrayLearningRate.sc\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val INDArrayLearningRatePluginUrl = \"https://gist.githubusercontent.com/TerrorJack/118487016d7973d67feb489449dee156/raw/778bb1b68a664c752b0945111220326731310214/INDArrayLearningRate.sc\"\n",
    "interp.load(scala.io.Source.fromURL(new java.net.URL(INDArrayLearningRatePluginUrl)).mkString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// `interp.load` is a workaround for https://github.com/lihaoyi/Ammonite/issues/649 and https://github.com/scala/bug/issues/10390\n",
    "interp.load(\"\"\"\n",
    "  val hyperparameters = Factory[Builtins with INDArrayLearningRate].newInstance(learningRate = 0.1)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `softmax` classifier (softmax classifier is a neural network combined by `softmax` and a full connection), we first need to write softmax function, formula: ![](https://www.zhihu.com/equation?tex=f_j%28z%29%3D%5Cfrac%7Be%5E%7Bz_j%7D%7D%7B%5Csum_ke%5E%7Bz_k%7D%7D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.implicits._\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.INDArrayLayer\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36msoftmax\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.INDArrayLayer\n",
    "\n",
    "def softmax(scores: INDArrayLayer): INDArrayLayer = {\n",
    "  val expScores = hyperparameters.exp(scores)\n",
    "  expScores / expScores.sum(1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose your  neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a full connection layer and [initialize Weight](https://github.com/ThoughtWorksInc/DeepLearning.scala/wiki/Getting-Started#231--weight-intialization), `Weight` shall be a two-dimension `INDArray` of `NumberOfPixels × NumberOfClasses`. `scores` is the score of each image corresponding to each category, representing the feasible probability of each category corresponding to each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mNumberOfClasses\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m\n",
       "\u001b[36mNumberOfPixels\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m3072\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//10 label of CIFAR10 images(airplane,automobile,bird,cat,deer,dog,frog,horse,ship,truck)\n",
    "val NumberOfClasses: Int = 10\n",
    "val NumberOfPixels: Int = 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.INDArrayWeight\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mweight\u001b[39m: \u001b[32mObject\u001b[39m with \u001b[32mhyperparameters\u001b[39m.\u001b[32mINDArrayWeightApi\u001b[39m with \u001b[32mhyperparameters\u001b[39m.\u001b[32mWeightApi\u001b[39m with \u001b[32mhyperparameters\u001b[39m.\u001b[32mWeightApi\u001b[39m = Weight[fullName=$sess.cmd9Wrapper.Helper.weight]\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mmyNeuralNetwork\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.INDArrayWeight\n",
    "\n",
    "val weight = {\n",
    "    import org.nd4s.Implicits._\n",
    "    INDArrayWeight(Nd4j.randn(NumberOfPixels, NumberOfClasses) * 0.001)\n",
    "}\n",
    "\n",
    "def myNeuralNetwork(input: INDArray): INDArrayLayer = {\n",
    "    softmax(input.dot(weight))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LossFunction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn about the prediction result of the neural network, we need to write the loss function `lossFunction`. We use [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy) to make comparison between this result and the actual result before return the score. Formula:\n",
    "![](https://zhihu.com/equation?tex=%5Cdisplaystyle+H%28p%2Cq%29%3D-%5Csum_xp%28x%29+logq%28x%29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mhyperparameters.DoubleLayer\n",
       "\n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36mlossFunction\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hyperparameters.DoubleLayer\n",
    "\n",
    "def lossFunction(input: INDArray, expectOutput: INDArray): DoubleLayer = {\n",
    "    val probabilities = myNeuralNetwork(input)\n",
    "    -(hyperparameters.log(probabilities) * expectOutput).mean\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read the images and corresponding label information for test data from CIFAR10 database and process them, we need [`import $file.ReadCIFAR10ToNDArray`](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/ReadCIFAR10ToNDArray.sc). This is a script file containing the read and processed CIFAR10 data, provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/demo/ReadCIFAR10ToNDArray.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$url.$                                                                                                                                           \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtrainNDArray\u001b[39m: \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32mHNil\u001b[39m]] = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestNDArray\u001b[39m: \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32m::\u001b[39m[\u001b[32mINDArray\u001b[39m, \u001b[32mshapeless\u001b[39m.\u001b[32mHNil\u001b[39m]] = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/demo/ReadCIFAR10ToNDArray.sc` => ReadCIFAR10ToNDArray}\n",
    "\n",
    "val trainNDArray = ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/data_batch_1.bin\", 1000)\n",
    "\n",
    "val testNDArray = ReadCIFAR10ToNDArray.readFromResource(\"/cifar-10-batches-bin/test_batch.bin\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing data to the softmax classifier, we first process label data with ([one hot encoding](https://en.wikipedia.org/wiki/One-hot)): transform INDArray of `NumberOfPixels × 1` into INDArray of `NumberOfPixels × NumberOfClasses`. The value of correct classification corresponding to each line is 1, and the values of other columns are 0. The reason for differentiating the training set and test set is to make it clear that whether the network is over trained which leads to [overfitting](https://en.wikipedia.org/wiki/Overfitting). While processing label data, we used [Utils](https://github.com/ThoughtWorksInc/DeepLearning.scala-website/blob/master/ipynbs/Utils.sc), which is also provided in this course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/demo/Utils.sc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtrainData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.23, 0.17, 0.20, 0.27, 0.38, 0.46, 0.54, 0.57, 0.58, 0.58, 0.51, 0.49, 0.55, 0.56, 0.54, 0.50, 0.54, 0.52, 0.48, 0.54, 0.54, 0.52, 0.53, 0.54, 0.59, 0.64, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestData\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.62, 0.62, 0.64, 0.65, 0.62, 0.61, 0.63, 0.62, 0.62, 0.62, 0.63, 0.62, 0.63, 0.65, 0.66, 0.66, 0.65, 0.63, 0.62, 0.62, 0.61, 0.58, 0.59, 0.58, 0.58, 0.56, 0.\u001b[33m...\u001b[39m\n",
       "\u001b[36mtrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [6.00, 9.00, 9.00, 4.00, 1.00, 1.00, 2.00, 7.00, 8.00, 3.00, 4.00, 7.00, 7.00, 2.00, 9.00, 9.00, 9.00, 3.00, 2.00, 6.00, 4.00, 3.00, 6.00, 6.00, 2.00, 6.00, 3.0\u001b[33m...\u001b[39m\n",
       "\u001b[36mtestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [3.00, 8.00, 8.00, 0.00, 6.00, 6.00, 1.00, 6.00, 3.00, 1.00, 0.00, 9.00, 5.00, 7.00, 9.00, 8.00, 5.00, 7.00, 8.00, 6.00, 7.00, 0.00, 4.00, 9.00, 5.00, 2.00, 4.0\u001b[33m...\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$url.$                                                                                                             \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mvectorizedTrainExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00],\n",
       "\u001b[33m...\u001b[39m\n",
       "\u001b[36mvectorizedTestExpectResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.00, 0.00, 0.00, 1.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00],\n",
       " [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 1.00, 0.00],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainData = trainNDArray.head\n",
    "val testData = testNDArray.head\n",
    "\n",
    "val trainExpectResult = trainNDArray.tail.head\n",
    "val testExpectResult = testNDArray.tail.head\n",
    "\n",
    "import $url.{`https://raw.githubusercontent.com/ThoughtWorksInc/DeepLearning.scala-website/master/demo/Utils.sc` => Utils}\n",
    "\n",
    "val vectorizedTrainExpectResult = Utils.makeVectorized(trainExpectResult, NumberOfClasses)\n",
    "val vectorizedTestExpectResult = Utils.makeVectorized(testExpectResult, NumberOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To observe the training process of the neural network, we need to output `loss`; while training the neural network, the `loss` shall be decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlossSeq\u001b[39m: \u001b[32mIndexedSeq\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mVector\u001b[39m()\n",
       "\u001b[36mtrainTask\u001b[39m: \u001b[32mFuture\u001b[39m[\u001b[32mUnit\u001b[39m] = scalaz.IndexedContsT@799d0e29"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var lossSeq: IndexedSeq[Double] = IndexedSeq.empty\n",
    "\n",
    "@monadic[Future]\n",
    "val trainTask: Future[Unit] = {\n",
    "  val lossStream = for (_ <- (1 to 2000).toStream) yield {\n",
    "    lossFunction(trainData, vectorizedTrainExpectResult).train.each\n",
    "  }\n",
    "  lossSeq = IndexedSeq.concat(lossStream)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict  your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the processed test data to verify the prediction result of the neural network and compute the accuracy. The accuracy shall be about 32%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Await.result(trainTask.asScala, Duration.Inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 32.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpredictResult\u001b[39m: \u001b[32mINDArray\u001b[39m = [[0.03, 0.05, 0.17, 0.13, 0.01, 0.13, 0.42, 0.00, 0.04, 0.00],\n",
       " [0.03, 0.17, 0.00, 0.05, 0.00, 0.01, 0.00, 0.00, 0.18, 0.55],\n",
       "\u001b[33m...\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictResult = Await.result(myNeuralNetwork(testData).predict.asScala, Duration.Inf)\n",
    "println(\"The accuracy is \" + Utils.getAccuracy(predictResult,testExpectResult) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <script type=\"text/javascript\">\n",
       "        require.config({\n",
       "  paths: {\n",
       "    d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.17/d3.min',\n",
       "    plotly: 'https://cdn.plot.ly/plotly-1.12.0.min'\n",
       "  },\n",
       "\n",
       "  shim: {\n",
       "    plotly: {\n",
       "      deps: ['d3', 'jquery'],\n",
       "      exports: 'plotly'\n",
       "    }\n",
       "  }\n",
       "});\n",
       "        \n",
       "\n",
       "        require(['plotly'], function(Plotly) {\n",
       "          window.Plotly = Plotly;\n",
       "        });\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"chart\" id=\"plot-1074041012\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "requirejs([\"plotly\"], function(Plotly) {\n",
       "  (function () {\n",
       "  var data0 = {\"type\":\"scatter\",\"x\":[0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0,35.0,36.0,37.0,38.0,39.0,40.0,41.0,42.0,43.0,44.0,45.0,46.0,47.0,48.0,49.0,50.0,51.0,52.0,53.0,54.0,55.0,56.0,57.0,58.0,59.0,60.0,61.0,62.0,63.0,64.0,65.0,66.0,67.0,68.0,69.0,70.0,71.0,72.0,73.0,74.0,75.0,76.0,77.0,78.0,79.0,80.0,81.0,82.0,83.0,84.0,85.0,86.0,87.0,88.0,89.0,90.0,91.0,92.0,93.0,94.0,95.0,96.0,97.0,98.0,99.0,100.0,101.0,102.0,103.0,104.0,105.0,106.0,107.0,108.0,109.0,110.0,111.0,112.0,113.0,114.0,115.0,116.0,117.0,118.0,119.0,120.0,121.0,122.0,123.0,124.0,125.0,126.0,127.0,128.0,129.0,130.0,131.0,132.0,133.0,134.0,135.0,136.0,137.0,138.0,139.0,140.0,141.0,142.0,143.0,144.0,145.0,146.0,147.0,148.0,149.0,150.0,151.0,152.0,153.0,154.0,155.0,156.0,157.0,158.0,159.0,160.0,161.0,162.0,163.0,164.0,165.0,166.0,167.0,168.0,169.0,170.0,171.0,172.0,173.0,174.0,175.0,176.0,177.0,178.0,179.0,180.0,181.0,182.0,183.0,184.0,185.0,186.0,187.0,188.0,189.0,190.0,191.0,192.0,193.0,194.0,195.0,196.0,197.0,198.0,199.0,200.0,201.0,202.0,203.0,204.0,205.0,206.0,207.0,208.0,209.0,210.0,211.0,212.0,213.0,214.0,215.0,216.0,217.0,218.0,219.0,220.0,221.0,222.0,223.0,224.0,225.0,226.0,227.0,228.0,229.0,230.0,231.0,232.0,233.0,234.0,235.0,236.0,237.0,238.0,239.0,240.0,241.0,242.0,243.0,244.0,245.0,246.0,247.0,248.0,249.0,250.0,251.0,252.0,253.0,254.0,255.0,256.0,257.0,258.0,259.0,260.0,261.0,262.0,263.0,264.0,265.0,266.0,267.0,268.0,269.0,270.0,271.0,272.0,273.0,274.0,275.0,276.0,277.0,278.0,279.0,280.0,281.0,282.0,283.0,284.0,285.0,286.0,287.0,288.0,289.0,290.0,291.0,292.0,293.0,294.0,295.0,296.0,297.0,298.0,299.0,300.0,301.0,302.0,303.0,304.0,305.0,306.0,307.0,308.0,309.0,310.0,311.0,312.0,313.0,314.0,315.0,316.0,317.0,318.0,319.0,320.0,321.0,322.0,323.0,324.0,325.0,326.0,327.0,328.0,329.0,330.0,331.0,332.0,333.0,334.0,335.0,336.0,337.0,338.0,339.0,340.0,341.0,342.0,343.0,344.0,345.0,346.0,347.0,348.0,349.0,350.0,351.0,352.0,353.0,354.0,355.0,356.0,357.0,358.0,359.0,360.0,361.0,362.0,363.0,364.0,365.0,366.0,367.0,368.0,369.0,370.0,371.0,372.0,373.0,374.0,375.0,376.0,377.0,378.0,379.0,380.0,381.0,382.0,383.0,384.0,385.0,386.0,387.0,388.0,389.0,390.0,391.0,392.0,393.0,394.0,395.0,396.0,397.0,398.0,399.0,400.0,401.0,402.0,403.0,404.0,405.0,406.0,407.0,408.0,409.0,410.0,411.0,412.0,413.0,414.0,415.0,416.0,417.0,418.0,419.0,420.0,421.0,422.0,423.0,424.0,425.0,426.0,427.0,428.0,429.0,430.0,431.0,432.0,433.0,434.0,435.0,436.0,437.0,438.0,439.0,440.0,441.0,442.0,443.0,444.0,445.0,446.0,447.0,448.0,449.0,450.0,451.0,452.0,453.0,454.0,455.0,456.0,457.0,458.0,459.0,460.0,461.0,462.0,463.0,464.0,465.0,466.0,467.0,468.0,469.0,470.0,471.0,472.0,473.0,474.0,475.0,476.0,477.0,478.0,479.0,480.0,481.0,482.0,483.0,484.0,485.0,486.0,487.0,488.0,489.0,490.0,491.0,492.0,493.0,494.0,495.0,496.0,497.0,498.0,499.0,500.0,501.0,502.0,503.0,504.0,505.0,506.0,507.0,508.0,509.0,510.0,511.0,512.0,513.0,514.0,515.0,516.0,517.0,518.0,519.0,520.0,521.0,522.0,523.0,524.0,525.0,526.0,527.0,528.0,529.0,530.0,531.0,532.0,533.0,534.0,535.0,536.0,537.0,538.0,539.0,540.0,541.0,542.0,543.0,544.0,545.0,546.0,547.0,548.0,549.0,550.0,551.0,552.0,553.0,554.0,555.0,556.0,557.0,558.0,559.0,560.0,561.0,562.0,563.0,564.0,565.0,566.0,567.0,568.0,569.0,570.0,571.0,572.0,573.0,574.0,575.0,576.0,577.0,578.0,579.0,580.0,581.0,582.0,583.0,584.0,585.0,586.0,587.0,588.0,589.0,590.0,591.0,592.0,593.0,594.0,595.0,596.0,597.0,598.0,599.0,600.0,601.0,602.0,603.0,604.0,605.0,606.0,607.0,608.0,609.0,610.0,611.0,612.0,613.0,614.0,615.0,616.0,617.0,618.0,619.0,620.0,621.0,622.0,623.0,624.0,625.0,626.0,627.0,628.0,629.0,630.0,631.0,632.0,633.0,634.0,635.0,636.0,637.0,638.0,639.0,640.0,641.0,642.0,643.0,644.0,645.0,646.0,647.0,648.0,649.0,650.0,651.0,652.0,653.0,654.0,655.0,656.0,657.0,658.0,659.0,660.0,661.0,662.0,663.0,664.0,665.0,666.0,667.0,668.0,669.0,670.0,671.0,672.0,673.0,674.0,675.0,676.0,677.0,678.0,679.0,680.0,681.0,682.0,683.0,684.0,685.0,686.0,687.0,688.0,689.0,690.0,691.0,692.0,693.0,694.0,695.0,696.0,697.0,698.0,699.0,700.0,701.0,702.0,703.0,704.0,705.0,706.0,707.0,708.0,709.0,710.0,711.0,712.0,713.0,714.0,715.0,716.0,717.0,718.0,719.0,720.0,721.0,722.0,723.0,724.0,725.0,726.0,727.0,728.0,729.0,730.0,731.0,732.0,733.0,734.0,735.0,736.0,737.0,738.0,739.0,740.0,741.0,742.0,743.0,744.0,745.0,746.0,747.0,748.0,749.0,750.0,751.0,752.0,753.0,754.0,755.0,756.0,757.0,758.0,759.0,760.0,761.0,762.0,763.0,764.0,765.0,766.0,767.0,768.0,769.0,770.0,771.0,772.0,773.0,774.0,775.0,776.0,777.0,778.0,779.0,780.0,781.0,782.0,783.0,784.0,785.0,786.0,787.0,788.0,789.0,790.0,791.0,792.0,793.0,794.0,795.0,796.0,797.0,798.0,799.0,800.0,801.0,802.0,803.0,804.0,805.0,806.0,807.0,808.0,809.0,810.0,811.0,812.0,813.0,814.0,815.0,816.0,817.0,818.0,819.0,820.0,821.0,822.0,823.0,824.0,825.0,826.0,827.0,828.0,829.0,830.0,831.0,832.0,833.0,834.0,835.0,836.0,837.0,838.0,839.0,840.0,841.0,842.0,843.0,844.0,845.0,846.0,847.0,848.0,849.0,850.0,851.0,852.0,853.0,854.0,855.0,856.0,857.0,858.0,859.0,860.0,861.0,862.0,863.0,864.0,865.0,866.0,867.0,868.0,869.0,870.0,871.0,872.0,873.0,874.0,875.0,876.0,877.0,878.0,879.0,880.0,881.0,882.0,883.0,884.0,885.0,886.0,887.0,888.0,889.0,890.0,891.0,892.0,893.0,894.0,895.0,896.0,897.0,898.0,899.0,900.0,901.0,902.0,903.0,904.0,905.0,906.0,907.0,908.0,909.0,910.0,911.0,912.0,913.0,914.0,915.0,916.0,917.0,918.0,919.0,920.0,921.0,922.0,923.0,924.0,925.0,926.0,927.0,928.0,929.0,930.0,931.0,932.0,933.0,934.0,935.0,936.0,937.0,938.0,939.0,940.0,941.0,942.0,943.0,944.0,945.0,946.0,947.0,948.0,949.0,950.0,951.0,952.0,953.0,954.0,955.0,956.0,957.0,958.0,959.0,960.0,961.0,962.0,963.0,964.0,965.0,966.0,967.0,968.0,969.0,970.0,971.0,972.0,973.0,974.0,975.0,976.0,977.0,978.0,979.0,980.0,981.0,982.0,983.0,984.0,985.0,986.0,987.0,988.0,989.0,990.0,991.0,992.0,993.0,994.0,995.0,996.0,997.0,998.0,999.0,1000.0,1001.0,1002.0,1003.0,1004.0,1005.0,1006.0,1007.0,1008.0,1009.0,1010.0,1011.0,1012.0,1013.0,1014.0,1015.0,1016.0,1017.0,1018.0,1019.0,1020.0,1021.0,1022.0,1023.0,1024.0,1025.0,1026.0,1027.0,1028.0,1029.0,1030.0,1031.0,1032.0,1033.0,1034.0,1035.0,1036.0,1037.0,1038.0,1039.0,1040.0,1041.0,1042.0,1043.0,1044.0,1045.0,1046.0,1047.0,1048.0,1049.0,1050.0,1051.0,1052.0,1053.0,1054.0,1055.0,1056.0,1057.0,1058.0,1059.0,1060.0,1061.0,1062.0,1063.0,1064.0,1065.0,1066.0,1067.0,1068.0,1069.0,1070.0,1071.0,1072.0,1073.0,1074.0,1075.0,1076.0,1077.0,1078.0,1079.0,1080.0,1081.0,1082.0,1083.0,1084.0,1085.0,1086.0,1087.0,1088.0,1089.0,1090.0,1091.0,1092.0,1093.0,1094.0,1095.0,1096.0,1097.0,1098.0,1099.0,1100.0,1101.0,1102.0,1103.0,1104.0,1105.0,1106.0,1107.0,1108.0,1109.0,1110.0,1111.0,1112.0,1113.0,1114.0,1115.0,1116.0,1117.0,1118.0,1119.0,1120.0,1121.0,1122.0,1123.0,1124.0,1125.0,1126.0,1127.0,1128.0,1129.0,1130.0,1131.0,1132.0,1133.0,1134.0,1135.0,1136.0,1137.0,1138.0,1139.0,1140.0,1141.0,1142.0,1143.0,1144.0,1145.0,1146.0,1147.0,1148.0,1149.0,1150.0,1151.0,1152.0,1153.0,1154.0,1155.0,1156.0,1157.0,1158.0,1159.0,1160.0,1161.0,1162.0,1163.0,1164.0,1165.0,1166.0,1167.0,1168.0,1169.0,1170.0,1171.0,1172.0,1173.0,1174.0,1175.0,1176.0,1177.0,1178.0,1179.0,1180.0,1181.0,1182.0,1183.0,1184.0,1185.0,1186.0,1187.0,1188.0,1189.0,1190.0,1191.0,1192.0,1193.0,1194.0,1195.0,1196.0,1197.0,1198.0,1199.0,1200.0,1201.0,1202.0,1203.0,1204.0,1205.0,1206.0,1207.0,1208.0,1209.0,1210.0,1211.0,1212.0,1213.0,1214.0,1215.0,1216.0,1217.0,1218.0,1219.0,1220.0,1221.0,1222.0,1223.0,1224.0,1225.0,1226.0,1227.0,1228.0,1229.0,1230.0,1231.0,1232.0,1233.0,1234.0,1235.0,1236.0,1237.0,1238.0,1239.0,1240.0,1241.0,1242.0,1243.0,1244.0,1245.0,1246.0,1247.0,1248.0,1249.0,1250.0,1251.0,1252.0,1253.0,1254.0,1255.0,1256.0,1257.0,1258.0,1259.0,1260.0,1261.0,1262.0,1263.0,1264.0,1265.0,1266.0,1267.0,1268.0,1269.0,1270.0,1271.0,1272.0,1273.0,1274.0,1275.0,1276.0,1277.0,1278.0,1279.0,1280.0,1281.0,1282.0,1283.0,1284.0,1285.0,1286.0,1287.0,1288.0,1289.0,1290.0,1291.0,1292.0,1293.0,1294.0,1295.0,1296.0,1297.0,1298.0,1299.0,1300.0,1301.0,1302.0,1303.0,1304.0,1305.0,1306.0,1307.0,1308.0,1309.0,1310.0,1311.0,1312.0,1313.0,1314.0,1315.0,1316.0,1317.0,1318.0,1319.0,1320.0,1321.0,1322.0,1323.0,1324.0,1325.0,1326.0,1327.0,1328.0,1329.0,1330.0,1331.0,1332.0,1333.0,1334.0,1335.0,1336.0,1337.0,1338.0,1339.0,1340.0,1341.0,1342.0,1343.0,1344.0,1345.0,1346.0,1347.0,1348.0,1349.0,1350.0,1351.0,1352.0,1353.0,1354.0,1355.0,1356.0,1357.0,1358.0,1359.0,1360.0,1361.0,1362.0,1363.0,1364.0,1365.0,1366.0,1367.0,1368.0,1369.0,1370.0,1371.0,1372.0,1373.0,1374.0,1375.0,1376.0,1377.0,1378.0,1379.0,1380.0,1381.0,1382.0,1383.0,1384.0,1385.0,1386.0,1387.0,1388.0,1389.0,1390.0,1391.0,1392.0,1393.0,1394.0,1395.0,1396.0,1397.0,1398.0,1399.0,1400.0,1401.0,1402.0,1403.0,1404.0,1405.0,1406.0,1407.0,1408.0,1409.0,1410.0,1411.0,1412.0,1413.0,1414.0,1415.0,1416.0,1417.0,1418.0,1419.0,1420.0,1421.0,1422.0,1423.0,1424.0,1425.0,1426.0,1427.0,1428.0,1429.0,1430.0,1431.0,1432.0,1433.0,1434.0,1435.0,1436.0,1437.0,1438.0,1439.0,1440.0,1441.0,1442.0,1443.0,1444.0,1445.0,1446.0,1447.0,1448.0,1449.0,1450.0,1451.0,1452.0,1453.0,1454.0,1455.0,1456.0,1457.0,1458.0,1459.0,1460.0,1461.0,1462.0,1463.0,1464.0,1465.0,1466.0,1467.0,1468.0,1469.0,1470.0,1471.0,1472.0,1473.0,1474.0,1475.0,1476.0,1477.0,1478.0,1479.0,1480.0,1481.0,1482.0,1483.0,1484.0,1485.0,1486.0,1487.0,1488.0,1489.0,1490.0,1491.0,1492.0,1493.0,1494.0,1495.0,1496.0,1497.0,1498.0,1499.0,1500.0,1501.0,1502.0,1503.0,1504.0,1505.0,1506.0,1507.0,1508.0,1509.0,1510.0,1511.0,1512.0,1513.0,1514.0,1515.0,1516.0,1517.0,1518.0,1519.0,1520.0,1521.0,1522.0,1523.0,1524.0,1525.0,1526.0,1527.0,1528.0,1529.0,1530.0,1531.0,1532.0,1533.0,1534.0,1535.0,1536.0,1537.0,1538.0,1539.0,1540.0,1541.0,1542.0,1543.0,1544.0,1545.0,1546.0,1547.0,1548.0,1549.0,1550.0,1551.0,1552.0,1553.0,1554.0,1555.0,1556.0,1557.0,1558.0,1559.0,1560.0,1561.0,1562.0,1563.0,1564.0,1565.0,1566.0,1567.0,1568.0,1569.0,1570.0,1571.0,1572.0,1573.0,1574.0,1575.0,1576.0,1577.0,1578.0,1579.0,1580.0,1581.0,1582.0,1583.0,1584.0,1585.0,1586.0,1587.0,1588.0,1589.0,1590.0,1591.0,1592.0,1593.0,1594.0,1595.0,1596.0,1597.0,1598.0,1599.0,1600.0,1601.0,1602.0,1603.0,1604.0,1605.0,1606.0,1607.0,1608.0,1609.0,1610.0,1611.0,1612.0,1613.0,1614.0,1615.0,1616.0,1617.0,1618.0,1619.0,1620.0,1621.0,1622.0,1623.0,1624.0,1625.0,1626.0,1627.0,1628.0,1629.0,1630.0,1631.0,1632.0,1633.0,1634.0,1635.0,1636.0,1637.0,1638.0,1639.0,1640.0,1641.0,1642.0,1643.0,1644.0,1645.0,1646.0,1647.0,1648.0,1649.0,1650.0,1651.0,1652.0,1653.0,1654.0,1655.0,1656.0,1657.0,1658.0,1659.0,1660.0,1661.0,1662.0,1663.0,1664.0,1665.0,1666.0,1667.0,1668.0,1669.0,1670.0,1671.0,1672.0,1673.0,1674.0,1675.0,1676.0,1677.0,1678.0,1679.0,1680.0,1681.0,1682.0,1683.0,1684.0,1685.0,1686.0,1687.0,1688.0,1689.0,1690.0,1691.0,1692.0,1693.0,1694.0,1695.0,1696.0,1697.0,1698.0,1699.0,1700.0,1701.0,1702.0,1703.0,1704.0,1705.0,1706.0,1707.0,1708.0,1709.0,1710.0,1711.0,1712.0,1713.0,1714.0,1715.0,1716.0,1717.0,1718.0,1719.0,1720.0,1721.0,1722.0,1723.0,1724.0,1725.0,1726.0,1727.0,1728.0,1729.0,1730.0,1731.0,1732.0,1733.0,1734.0,1735.0,1736.0,1737.0,1738.0,1739.0,1740.0,1741.0,1742.0,1743.0,1744.0,1745.0,1746.0,1747.0,1748.0,1749.0,1750.0,1751.0,1752.0,1753.0,1754.0,1755.0,1756.0,1757.0,1758.0,1759.0,1760.0,1761.0,1762.0,1763.0,1764.0,1765.0,1766.0,1767.0,1768.0,1769.0,1770.0,1771.0,1772.0,1773.0,1774.0,1775.0,1776.0,1777.0,1778.0,1779.0,1780.0,1781.0,1782.0,1783.0,1784.0,1785.0,1786.0,1787.0,1788.0,1789.0,1790.0,1791.0,1792.0,1793.0,1794.0,1795.0,1796.0,1797.0,1798.0,1799.0,1800.0,1801.0,1802.0,1803.0,1804.0,1805.0,1806.0,1807.0,1808.0,1809.0,1810.0,1811.0,1812.0,1813.0,1814.0,1815.0,1816.0,1817.0,1818.0,1819.0,1820.0,1821.0,1822.0,1823.0,1824.0,1825.0,1826.0,1827.0,1828.0,1829.0,1830.0,1831.0,1832.0,1833.0,1834.0,1835.0,1836.0,1837.0,1838.0,1839.0,1840.0,1841.0,1842.0,1843.0,1844.0,1845.0,1846.0,1847.0,1848.0,1849.0,1850.0,1851.0,1852.0,1853.0,1854.0,1855.0,1856.0,1857.0,1858.0,1859.0,1860.0,1861.0,1862.0,1863.0,1864.0,1865.0,1866.0,1867.0,1868.0,1869.0,1870.0,1871.0,1872.0,1873.0,1874.0,1875.0,1876.0,1877.0,1878.0,1879.0,1880.0,1881.0,1882.0,1883.0,1884.0,1885.0,1886.0,1887.0,1888.0,1889.0,1890.0,1891.0,1892.0,1893.0,1894.0,1895.0,1896.0,1897.0,1898.0,1899.0,1900.0,1901.0,1902.0,1903.0,1904.0,1905.0,1906.0,1907.0,1908.0,1909.0,1910.0,1911.0,1912.0,1913.0,1914.0,1915.0,1916.0,1917.0,1918.0,1919.0,1920.0,1921.0,1922.0,1923.0,1924.0,1925.0,1926.0,1927.0,1928.0,1929.0,1930.0,1931.0,1932.0,1933.0,1934.0,1935.0,1936.0,1937.0,1938.0,1939.0,1940.0,1941.0,1942.0,1943.0,1944.0,1945.0,1946.0,1947.0,1948.0,1949.0,1950.0,1951.0,1952.0,1953.0,1954.0,1955.0,1956.0,1957.0,1958.0,1959.0,1960.0,1961.0,1962.0,1963.0,1964.0,1965.0,1966.0,1967.0,1968.0,1969.0,1970.0,1971.0,1972.0,1973.0,1974.0,1975.0,1976.0,1977.0,1978.0,1979.0,1980.0,1981.0,1982.0,1983.0,1984.0,1985.0,1986.0,1987.0,1988.0,1989.0,1990.0,1991.0,1992.0,1993.0,1994.0,1995.0,1996.0,1997.0,1998.0,1999.0],\"y\":[0.2304267333984375,0.2286988525390625,0.22771962890625,0.2268181884765625,0.2259482177734375,0.225104541015625,0.224285595703125,0.2234902587890625,0.22271748046875,0.2219664306640625,0.2212361328125,0.220525732421875,0.2198344482421875,0.219161474609375,0.2185060546875,0.21786748046875,0.217245068359375,0.216638232421875,0.2160462890625,0.2154686767578125,0.214904833984375,0.21435419921875,0.21381630859375,0.2132906982421875,0.21277685546875,0.2122743408203125,0.211782763671875,0.2113017333984375,0.2108307861328125,0.210369677734375,0.209918017578125,0.209475439453125,0.209041650390625,0.2086163818359375,0.20819931640625,0.2077901123046875,0.2073886474609375,0.20699453125,0.206607666015625,0.206227685546875,0.2058544921875,0.205487744140625,0.20512734375,0.2047730224609375,0.2044246337890625,0.20408201904296874,0.203744970703125,0.20341334228515626,0.2030869384765625,0.202765673828125,0.202449365234375,0.2021378662109375,0.2018310546875,0.20152877197265626,0.2012309326171875,0.2009374267578125,0.2006480712890625,0.2003628173828125,0.20008153076171875,0.1998041015625,0.19953045654296875,0.19926048583984374,0.198994091796875,0.19873121337890626,0.1984716796875,0.198215478515625,0.1979625732421875,0.1977127685546875,0.19746607666015625,0.1972223876953125,0.196981640625,0.1967437744140625,0.19650869140625,0.1962763916015625,0.196046728515625,0.1958197021484375,0.1955952392578125,0.195373291015625,0.19515377197265624,0.1949366455078125,0.19472191162109376,0.194509423828125,0.19429918212890626,0.1940911865234375,0.1938853271484375,0.1936815673828125,0.1934798828125,0.19328017578125,0.19308253173828124,0.1928867919921875,0.19269296875,0.192501025390625,0.1923108642578125,0.19212257080078124,0.19193599853515625,0.19175115966796874,0.1915680419921875,0.1913865478515625,0.19120673828125,0.19102852783203125,0.19085185546875,0.19067677001953126,0.1905031982421875,0.19033111572265626,0.19016051025390626,0.18999132080078124,0.18982357177734374,0.1896572021484375,0.1894922119140625,0.18932855224609374,0.1891662109375,0.1890052001953125,0.1888454345703125,0.188686962890625,0.18852967529296874,0.18837362060546875,0.18821878662109376,0.18806513671875,0.1879126220703125,0.187761279296875,0.18761104736328124,0.1874618896484375,0.18731385498046876,0.1871668701171875,0.187020947265625,0.18687608642578124,0.1867322509765625,0.1865893798828125,0.1864475341796875,0.1863066650390625,0.1861667724609375,0.186027783203125,0.18588980712890624,0.18575269775390624,0.1856165283203125,0.18548123779296874,0.1853468505859375,0.1852133056640625,0.18508065185546874,0.1849488525390625,0.18481787109375,0.1846877197265625,0.1845583984375,0.1844298828125,0.1843021484375,0.1841751953125,0.1840489990234375,0.1839236328125,0.18379893798828126,0.18367503662109375,0.18355185546875,0.18342939453125,0.1833076416015625,0.1831865966796875,0.18306624755859374,0.1829466064453125,0.1828276611328125,0.1827093505859375,0.18259169921875,0.18247471923828126,0.1823584228515625,0.1822427001953125,0.1821276611328125,0.182013232421875,0.1818994140625,0.18178621826171876,0.181673583984375,0.18156162109375,0.18145018310546876,0.18133935546875,0.1812291015625,0.1811194091796875,0.18101029052734374,0.180901708984375,0.1807937255859375,0.18068623046875,0.18057930908203124,0.180472900390625,0.18036702880859376,0.18026168212890625,0.1801568603515625,0.18005252685546874,0.1799487060546875,0.1798453857421875,0.17974256591796875,0.17964022216796874,0.17953837890625,0.17943701171875,0.179336083984375,0.1792356689453125,0.179135693359375,0.17903619384765626,0.1789371337890625,0.1788385498046875,0.1787404052734375,0.1786427001953125,0.1785453857421875,0.1784485595703125,0.1783521240234375,0.17825615234375,0.1781605712890625,0.1780654052734375,0.17797069091796874,0.1778763671875,0.17778243408203126,0.17768890380859376,0.177595751953125,0.17750303955078126,0.1774106689453125,0.177318701171875,0.17722713623046876,0.17713592529296876,0.1770451171875,0.1769546630859375,0.17686456298828124,0.1767748291015625,0.17668548583984375,0.176596484375,0.1765078369140625,0.1764195556640625,0.17633160400390624,0.176243994140625,0.17615673828125,0.17606983642578125,0.17598323974609376,0.1758969970703125,0.17581109619140625,0.17572548828125,0.1756402587890625,0.175555322265625,0.175470703125,0.1753864013671875,0.1753024169921875,0.17521875,0.1751353759765625,0.17505234375,0.17496961669921876,0.174887158203125,0.17480499267578126,0.17472315673828126,0.1746416015625,0.17456031494140625,0.17447935791015626,0.17439866943359375,0.17431826171875,0.1742381591796875,0.1741583251953125,0.174078759765625,0.173999462890625,0.173920458984375,0.1738417236328125,0.1737632568359375,0.1736850830078125,0.17360714111328124,0.17352950439453124,0.17345206298828125,0.17337493896484374,0.173298046875,0.17322142333984375,0.1731450439453125,0.1730689208984375,0.1729930419921875,0.1729174072265625,0.172842041015625,0.17276690673828124,0.1726920166015625,0.1726173583984375,0.1725429443359375,0.1724687744140625,0.17239482421875,0.1723211181640625,0.17224764404296874,0.1721744140625,0.17210140380859376,0.17202862548828124,0.1719560791015625,0.17188372802734375,0.17181163330078125,0.17173974609375,0.17166807861328126,0.17159661865234374,0.17152540283203124,0.171454345703125,0.17138355712890624,0.1713129638671875,0.171242578125,0.1711723876953125,0.17110242919921875,0.1710326904296875,0.1709631103515625,0.1708937744140625,0.17082459716796874,0.1707556640625,0.17068692626953125,0.17061837158203125,0.1705500244140625,0.1704818603515625,0.17041390380859375,0.17034615478515625,0.170278564453125,0.17021119384765626,0.1701439697265625,0.17007696533203126,0.17001014404296874,0.16994349365234376,0.16987705078125,0.16981080322265624,0.1697447021484375,0.16967882080078125,0.1696130859375,0.16954752197265624,0.16948216552734374,0.16941695556640626,0.1693519287109375,0.1692870849609375,0.169222412109375,0.16915794677734375,0.169093603515625,0.169029443359375,0.16896546630859374,0.16890166015625,0.16883800048828124,0.16877451171875,0.168711181640625,0.16864803466796874,0.16858502197265626,0.16852220458984374,0.1684595458984375,0.168397021484375,0.16833468017578124,0.1682724609375,0.1682104248046875,0.16814857177734374,0.16808682861328125,0.1680252685546875,0.1679638427734375,0.1679025634765625,0.167841455078125,0.167780517578125,0.1677197021484375,0.1676590576171875,0.1675985107421875,0.1675381591796875,0.1674779541015625,0.1674178466796875,0.16735791015625,0.16729813232421875,0.1672385009765625,0.16717899169921874,0.16711964111328126,0.1670604248046875,0.1670013427734375,0.1669424072265625,0.1668836181640625,0.166824951171875,0.1667664306640625,0.1667080322265625,0.1666498046875,0.16659169921875,0.16653369140625,0.1664758544921875,0.16641812744140624,0.16636053466796874,0.1663031005859375,0.16624576416015624,0.16618856201171875,0.166131494140625,0.16607457275390625,0.16601778564453126,0.165961083984375,0.1659045166015625,0.165848095703125,0.1657917724609375,0.1657356201171875,0.1656795654296875,0.1656236083984375,0.165567822265625,0.16551212158203124,0.1654565673828125,0.16540113525390626,0.16534580078125,0.16529058837890626,0.1652355224609375,0.1651805419921875,0.16512568359375,0.16507095947265624,0.1650163330078125,0.16496181640625,0.16490743408203126,0.16485316162109376,0.16479901123046875,0.164744970703125,0.1646910400390625,0.16463720703125,0.16458349609375,0.16452989501953125,0.164476416015625,0.16442303466796876,0.164369775390625,0.1643166015625,0.1642635498046875,0.16421063232421876,0.1641577880859375,0.1641050537109375,0.16405244140625,0.1639999267578125,0.1639475341796875,0.1638952392578125,0.16384302978515625,0.16379093017578125,0.163738916015625,0.16368704833984374,0.16363525390625,0.1635835693359375,0.163531982421875,0.163480517578125,0.1634291259765625,0.1633778564453125,0.16332666015625,0.16327557373046875,0.16322459716796875,0.1631737060546875,0.1631229248046875,0.16307222900390625,0.1630216552734375,0.162971142578125,0.1629207275390625,0.1628704345703125,0.16282022705078125,0.1627701171875,0.16272008056640624,0.1626701416015625,0.1626203125,0.162570556640625,0.16252093505859375,0.1624713623046875,0.162421923828125,0.1623725341796875,0.16232325439453124,0.16227406005859374,0.16222496337890624,0.1621759521484375,0.1621270263671875,0.1620781982421875,0.162029443359375,0.161980810546875,0.16193221435546876,0.1618837158203125,0.1618353271484375,0.1617870361328125,0.16173883056640626,0.161690673828125,0.16164263916015625,0.161594677734375,0.16154677734375,0.16149898681640626,0.16145128173828124,0.161403662109375,0.16135611572265626,0.1613086181640625,0.161261279296875,0.16121396484375,0.16116673583984376,0.16111961669921876,0.16107255859375,0.1610255859375,0.16097872314453124,0.1609319091796875,0.16088516845703124,0.160838525390625,0.16079195556640624,0.16074544677734376,0.1606990234375,0.1606527099609375,0.16060643310546874,0.1605602783203125,0.16051416015625,0.1604681640625,0.160422216796875,0.16037633056640624,0.1603305419921875,0.16028482666015625,0.1602391845703125,0.1601936279296875,0.1601481201171875,0.160102734375,0.16005736083984376,0.160012109375,0.15996689453125,0.15992177734375,0.15987672119140625,0.15983173828125,0.15978685302734374,0.15974202880859376,0.159697265625,0.15965257568359376,0.159607958984375,0.1595634033203125,0.1595189453125,0.1594745361328125,0.1594302001953125,0.15938594970703124,0.159341748046875,0.15929761962890626,0.159253564453125,0.1592095947265625,0.1591656494140625,0.159121826171875,0.1590780517578125,0.1590343505859375,0.1589906982421875,0.15894710693359376,0.15890362548828124,0.1588601806640625,0.158816796875,0.15877349853515624,0.1587302490234375,0.1586870849609375,0.1586439697265625,0.158600927734375,0.1585579345703125,0.15851502685546875,0.1584721923828125,0.15842939453125,0.15838671875,0.15834404296875,0.1583014404296875,0.1582589111328125,0.158216455078125,0.15817403564453125,0.1581317138671875,0.15808944091796875,0.1580472412109375,0.15800509033203125,0.15796298828125,0.1579209716796875,0.1578790283203125,0.15783709716796876,0.15779527587890624,0.1577534912109375,0.157711767578125,0.15767010498046874,0.157628515625,0.157586962890625,0.1575454833984375,0.157504052734375,0.15746270751953126,0.15742142333984374,0.15738018798828124,0.1573389892578125,0.1572978759765625,0.1572568115234375,0.1572157958984375,0.157174853515625,0.1571339599609375,0.157093115234375,0.15705235595703124,0.1570116455078125,0.1569709716796875,0.1569303955078125,0.15688983154296876,0.1568493408203125,0.1568089111328125,0.15676851806640624,0.1567281982421875,0.156687939453125,0.15664774169921875,0.1566075927734375,0.15656749267578124,0.15652742919921875,0.156487451171875,0.1564474853515625,0.1564076416015625,0.15636781005859374,0.15632802734375,0.156288330078125,0.1562486572265625,0.15620904541015626,0.15616949462890625,0.15612998046875,0.15609053955078125,0.1560511474609375,0.1560117919921875,0.155972509765625,0.15593326416015624,0.1558940673828125,0.155854931640625,0.1558158447265625,0.155776806640625,0.155737841796875,0.15569892578125,0.1556600341796875,0.1556212158203125,0.155582470703125,0.1555437255859375,0.15550504150390626,0.1554664306640625,0.1554278564453125,0.1553893310546875,0.15535086669921874,0.155312451171875,0.1552740478515625,0.1552357421875,0.1551974609375,0.1551592529296875,0.1551210693359375,0.1550829345703125,0.15504488525390625,0.15500684814453125,0.15496888427734376,0.15493092041015624,0.1548930419921875,0.15485521240234376,0.1548174072265625,0.1547796875,0.1547419677734375,0.154704345703125,0.154666748046875,0.15462919921875,0.15459169921875,0.15455423583984376,0.1545168212890625,0.1544794677734375,0.15444215087890625,0.1544048828125,0.15436763916015625,0.15433046875,0.1542933349609375,0.15425626220703126,0.1542192138671875,0.1541822265625,0.1541452880859375,0.1541083740234375,0.1540715087890625,0.15403468017578126,0.15399791259765624,0.153961181640625,0.15392451171875,0.15388790283203124,0.1538512939453125,0.15381474609375,0.15377823486328124,0.1537417724609375,0.15370537109375,0.15366898193359374,0.15363267822265625,0.15359638671875,0.1535601318359375,0.15352393798828126,0.15348779296875,0.1534516845703125,0.153415625,0.1533795654296875,0.153343603515625,0.1533076416015625,0.1532717529296875,0.153235888671875,0.15320008544921876,0.1531643310546875,0.1531285888671875,0.1530928955078125,0.15305726318359375,0.1530216552734375,0.15298607177734375,0.152950537109375,0.1529150634765625,0.152879638671875,0.15284423828125,0.15280888671875,0.15277353515625,0.15273824462890626,0.15270302734375,0.1526678466796875,0.1526326904296875,0.15259755859375,0.1525624755859375,0.1525274658203125,0.15249248046875,0.1524574951171875,0.15242259521484375,0.15238770751953126,0.152352880859375,0.1523180908203125,0.1522833251953125,0.152248583984375,0.152213916015625,0.152179248046875,0.1521446533203125,0.1521100830078125,0.15207557373046876,0.15204110107421875,0.152006640625,0.15197222900390625,0.1519378662109375,0.151903515625,0.1518692138671875,0.1518349853515625,0.151800732421875,0.151766552734375,0.151732421875,0.151698291015625,0.1516642333984375,0.151630224609375,0.1515962158203125,0.151562255859375,0.1515283203125,0.15149443359375,0.15146058349609376,0.1514267822265625,0.15139302978515626,0.1513592529296875,0.15132557373046876,0.15129190673828125,0.151258251953125,0.1512246826171875,0.15119112548828126,0.1511575927734375,0.15112410888671876,0.151090673828125,0.1510572509765625,0.15102388916015624,0.15099053955078126,0.1509572021484375,0.15092396240234374,0.15089073486328125,0.15085753173828126,0.1508243408203125,0.15079119873046876,0.1507581298828125,0.150725048828125,0.1506920166015625,0.1506590576171875,0.15062608642578126,0.1505931640625,0.1505602783203125,0.15052742919921874,0.15049459228515624,0.1504617919921875,0.150429052734375,0.150396337890625,0.1503636474609375,0.150331005859375,0.1502983642578125,0.1502657958984375,0.1502332275390625,0.150200732421875,0.1501682373046875,0.150135791015625,0.15010338134765625,0.15007099609375,0.150038671875,0.15000634765625,0.14997406005859376,0.1499418212890625,0.1499095703125,0.14987740478515624,0.1498452392578125,0.14981312255859375,0.1497810302734375,0.14974898681640625,0.14971695556640624,0.1496849853515625,0.14965301513671875,0.1496210693359375,0.14958917236328126,0.14955732421875,0.14952548828125,0.149493701171875,0.14946192626953125,0.14943017578125,0.14939847412109375,0.1493668212890625,0.14933516845703124,0.14930355224609376,0.14927196044921875,0.1492404052734375,0.1492089111328125,0.1491774169921875,0.14914595947265624,0.14911456298828124,0.149083154296875,0.14905179443359376,0.149020458984375,0.14898914794921875,0.14895789794921874,0.1489266357421875,0.14889544677734376,0.1488642578125,0.14883310546875,0.1488019775390625,0.1487708984375,0.14873985595703124,0.1487088134765625,0.1486778076171875,0.1486468505859375,0.14861590576171874,0.14858497314453126,0.1485541015625,0.1485232666015625,0.148492431640625,0.14846163330078124,0.14843087158203125,0.14840015869140624,0.14836944580078126,0.14833876953125,0.1483081298828125,0.1482775146484375,0.148246923828125,0.148216357421875,0.14818583984375,0.1481553466796875,0.1481248779296875,0.14809443359375,0.148064013671875,0.1480336181640625,0.1480032470703125,0.1479729248046875,0.147942626953125,0.147912353515625,0.1478821044921875,0.14785191650390625,0.14782171630859375,0.147791552734375,0.14776142578125,0.147731298828125,0.14770123291015624,0.14767119140625,0.147641162109375,0.1476111572265625,0.147581201171875,0.14755125732421875,0.14752135009765624,0.14749149169921874,0.14746162109375,0.1474318115234375,0.14740198974609375,0.147372216796875,0.14734248046875,0.14731275634765625,0.14728304443359375,0.1472533935546875,0.14722376708984375,0.147194140625,0.1471645751953125,0.1471349853515625,0.1471054931640625,0.1470759521484375,0.147046484375,0.14701702880859374,0.14698760986328124,0.146958203125,0.14692886962890625,0.14689949951171874,0.146870166015625,0.146840869140625,0.14681162109375,0.14678238525390624,0.146753173828125,0.14672396240234375,0.1466947998046875,0.1466656494140625,0.146636572265625,0.14660748291015624,0.14657843017578126,0.1465493896484375,0.14652037353515626,0.14649140625,0.1464624267578125,0.1464335205078125,0.1464045654296875,0.146375732421875,0.14634686279296874,0.1463180419921875,0.14628922119140625,0.14626044921875,0.14623167724609376,0.1462029541015625,0.1461742431640625,0.146145556640625,0.14611690673828126,0.14608828125,0.1460596923828125,0.14603109130859376,0.1460025390625,0.1459739990234375,0.1459454833984375,0.1459170166015625,0.1458885498046875,0.145860107421875,0.14583173828125,0.1458033203125,0.14577496337890625,0.145746630859375,0.145718310546875,0.1456900146484375,0.145661767578125,0.14563349609375,0.14560531005859376,0.14557711181640626,0.14554893798828125,0.14552080078125,0.14549267578125,0.145464599609375,0.1454364990234375,0.14540845947265624,0.1453804443359375,0.14535244140625,0.145324462890625,0.14529649658203125,0.1452685546875,0.145240673828125,0.1452127685546875,0.145184912109375,0.145157080078125,0.14512926025390624,0.1451014892578125,0.1450737060546875,0.145045947265625,0.14501822509765624,0.14499052734375,0.14496285400390624,0.144935205078125,0.144907568359375,0.14487996826171876,0.1448523681640625,0.1448248046875,0.14479730224609375,0.1447697509765625,0.1447422607421875,0.144714794921875,0.144687353515625,0.144659912109375,0.14463251953125,0.144605126953125,0.14457777099609376,0.14455042724609374,0.144523095703125,0.14449583740234376,0.1444685546875,0.14444130859375,0.1444140869140625,0.1443868896484375,0.14435966796875,0.1443325439453125,0.1443053955078125,0.14427828369140624,0.1442511962890625,0.1442240966796875,0.14419705810546876,0.14417001953125,0.144143017578125,0.144116015625,0.1440890625,0.14406212158203124,0.14403519287109376,0.1440083251953125,0.14398140869140624,0.14395457763671876,0.14392774658203125,0.143900927734375,0.14387412109375,0.14384736328125,0.1438205810546875,0.14379385986328125,0.1437671630859375,0.143740478515625,0.143713818359375,0.14368717041015625,0.1436605224609375,0.143633935546875,0.14360736083984374,0.143580810546875,0.14355426025390625,0.143527734375,0.1435012451171875,0.14347476806640624,0.143448291015625,0.143421875,0.143395458984375,0.1433690673828125,0.14334268798828126,0.1433163330078125,0.143289990234375,0.1432636962890625,0.1432373779296875,0.1432111328125,0.14318486328125,0.143158642578125,0.143132421875,0.1431062255859375,0.143080078125,0.14305390625,0.143027783203125,0.14300166015625,0.1429755615234375,0.1429494873046875,0.1429234619140625,0.142897412109375,0.1428714111328125,0.14284541015625,0.14281943359375,0.142793505859375,0.1427675537109375,0.14274163818359376,0.1427157470703125,0.1426898681640625,0.142664013671875,0.1426382080078125,0.14261236572265626,0.142586572265625,0.1425608154296875,0.14253505859375,0.14250931396484376,0.14248359375,0.1424578857421875,0.1424322265625,0.14240654296875,0.142380908203125,0.1423552978515625,0.1423296875,0.1423041259765625,0.14227855224609376,0.14225302734375,0.14222747802734376,0.1422019775390625,0.1421764892578125,0.14215101318359374,0.1421255615234375,0.142100146484375,0.1420747314453125,0.14204932861328126,0.1420239501953125,0.1419986328125,0.1419732666015625,0.14194794921875,0.14192265625,0.1418973876953125,0.141872119140625,0.141846875,0.1418216552734375,0.141796435546875,0.141771240234375,0.14174609375,0.1417209228515625,0.14169578857421875,0.1416706787109375,0.141645556640625,0.1416205078125,0.14159544677734376,0.14157039794921875,0.1415453857421875,0.141520361328125,0.14149539794921875,0.14147041015625,0.14144547119140624,0.1414205322265625,0.14139561767578124,0.1413707275390625,0.14134586181640624,0.14132098388671874,0.141296142578125,0.1412713134765625,0.1412465087890625,0.141221728515625,0.1411969482421875,0.1411721923828125,0.14114747314453124,0.14112275390625,0.1410980224609375,0.1410733642578125,0.141048681640625,0.14102406005859375,0.1409994140625,0.1409748046875,0.1409501953125,0.1409256103515625,0.14090106201171876,0.14087652587890626,0.14085198974609375,0.140827490234375,0.140802978515625,0.14077852783203126,0.14075406494140624,0.1407296142578125,0.1407052001953125,0.14068079833984376,0.14065640869140625,0.14063203125,0.14060770263671876,0.140583349609375,0.140559033203125,0.140534716796875,0.14051044921875,0.140486181640625,0.1404619140625,0.1404376708984375,0.14041346435546875,0.1403892822265625,0.140365087890625,0.14034091796875,0.14031676025390624,0.14029263916015625,0.1402685302734375,0.14024443359375,0.1402203369140625,0.1401962890625,0.1401721923828125,0.140148193359375,0.140124169921875,0.1401001708984375,0.140076171875,0.1400522216796875,0.140028271484375,0.140004345703125,0.139980419921875,0.1399565185546875,0.13993260498046875,0.13990875244140624,0.1398848876953125,0.1398610595703125,0.1398372314453125,0.13981343994140624,0.1397896484375,0.1397658935546875,0.13974210205078125,0.1397183837890625,0.13969464111328125,0.139670947265625,0.13964725341796874,0.139623583984375,0.13959990234375,0.13957626953125,0.1395526611328125,0.1395290283203125,0.1395054443359375,0.13948184814453124,0.13945828857421874,0.1394347412109375,0.13941123046875,0.13938770751953125,0.13936419677734374,0.1393406982421875,0.13931724853515626,0.139293798828125,0.139270361328125,0.13924693603515625,0.13922354736328124,0.139200146484375,0.1391767578125,0.13915340576171875,0.1391300537109375,0.13910672607421876,0.1390834228515625,0.139060107421875,0.1390368408203125,0.13901357421875,0.1389903076171875,0.1389670654296875,0.13894383544921876,0.1389206298828125,0.1388974609375,0.1388742919921875,0.138851123046875,0.1388279541015625,0.138804833984375,0.13878173828125,0.1387586181640625,0.13873553466796876,0.13871246337890625,0.138689404296875,0.13866636962890624,0.13864334716796875,0.13862034912109375,0.1385973388671875,0.138574365234375,0.138551416015625,0.13852845458984375,0.138505517578125,0.13848260498046874,0.13845970458984375,0.13843681640625,0.1384139404296875,0.138391064453125,0.13836822509765626,0.13834541015625,0.1383225830078125,0.1382997802734375,0.13827698974609376,0.13825421142578126,0.1382314697265625,0.13820872802734374,0.1381860107421875,0.13816328125,0.138140576171875,0.1381178955078125,0.13809521484375,0.13807257080078125,0.13804993896484374,0.13802730712890626,0.1380046875,0.13798209228515626,0.13795950927734374,0.13793692626953125,0.1379143798828125,0.137891845703125,0.1378693115234375,0.13784678955078125,0.13782430419921876,0.1378018310546875,0.137779345703125,0.137756884765625,0.1377344482421875,0.1377120361328125,0.1376896240234375,0.1376672119140625,0.13764483642578124,0.13762247314453124,0.1376001220703125,0.137577783203125,0.1375554443359375,0.1375331298828125,0.1375108154296875,0.13748853759765625,0.137466259765625,0.137443994140625,0.1374217529296875,0.1373995361328125,0.1373773193359375,0.1373551025390625,0.13733291015625,0.1373107421875,0.13728857421875,0.13726644287109374,0.1372443115234375,0.13722218017578125,0.13720009765625,0.137177978515625,0.13715589599609376,0.13713385009765625,0.137111767578125,0.13708974609375,0.137067724609375,0.137045703125,0.13702371826171875,0.1370017333984375,0.1369797607421875,0.13695780029296875,0.1369358642578125,0.13691392822265624,0.1368919921875,0.13687010498046875,0.1368482177734375,0.13682635498046875,0.1368044677734375,0.1367826171875,0.1367607666015625,0.13673895263671876,0.136717138671875,0.136695361328125,0.1366735595703125,0.1366517822265625,0.136630029296875,0.13660830078125,0.136586572265625,0.13656484375,0.1365431396484375,0.13652147216796875,0.1364997802734375,0.136478125,0.13645645751953125,0.136434814453125,0.13641318359375,0.13639158935546875,0.1363699951171875,0.136348388671875,0.13632681884765624,0.1363052490234375,0.13628369140625,0.1362621826171875,0.13624066162109374,0.13621915283203126,0.1361976318359375,0.136176171875,0.13615469970703126,0.1361332275390625,0.1361117919921875,0.1360903564453125,0.1360689453125,0.1360475341796875,0.136026123046875,0.13600474853515626,0.1359833740234375,0.1359620361328125,0.135940673828125,0.13591934814453124,0.1358980224609375,0.13587672119140626,0.13585543212890624,0.13583414306640626,0.13581287841796874,0.13579161376953125,0.135770361328125,0.13574913330078126,0.13572791748046875,0.1357067138671875,0.1356855224609375,0.1356643310546875,0.13564317626953126,0.1356219970703125,0.13560087890625,0.135579736328125,0.1355586181640625,0.13553753662109375,0.13551641845703125,0.1354953369140625,0.1354742431640625,0.1354531982421875,0.135432177734375,0.1354111328125,0.1353901123046875,0.1353691162109375,0.135348095703125,0.13532711181640625,0.1353061279296875,0.1352851806640625,0.1352642333984375,0.13524329833984375,0.1352223876953125,0.1352014404296875,0.13518056640625,0.13515966796875,0.135138818359375,0.1351179443359375,0.13509710693359375,0.1350762451171875,0.135055419921875,0.1350345947265625,0.13501380615234376,0.1349929931640625,0.1349722412109375,0.1349514404296875,0.134930712890625,0.1349099365234375,0.134889208984375,0.1348684814453125,0.13484779052734375,0.13482708740234375,0.134806396484375,0.1347857177734375,0.1347650634765625,0.1347444091796875,0.134723779296875,0.1347031494140625,0.13468251953125,0.1346619384765625,0.1346413330078125,0.1346207763671875,0.1346001953125,0.1345796142578125,0.13455906982421875,0.1345385498046875,0.13451802978515626,0.134497509765625,0.1344770263671875,0.1344565185546875,0.1344360595703125,0.134415576171875,0.1343951171875,0.13437470703125,0.13435426025390626,0.134333837890625,0.134313427734375,0.13429302978515625,0.13427265625,0.1342522705078125,0.1342319091796875,0.1342115478515625,0.1341912109375,0.1341708984375,0.13415057373046874,0.1341302734375,0.1341099609375,0.1340896728515625,0.13406942138671876,0.13404915771484374,0.13402890625,0.134008642578125,0.133988427734375,0.133968212890625,0.1339480224609375,0.1339278076171875,0.1339076416015625,0.1338874755859375,0.13386732177734376,0.13384716796875,0.13382703857421874,0.133806884765625,0.1337867919921875,0.13376668701171876,0.13374658203125,0.1337264892578125,0.1337064453125,0.133686376953125,0.13366630859375,0.1336462646484375,0.1336262451171875,0.1336062255859375,0.13358623046875,0.1335662353515625,0.13354625244140625,0.13352628173828124,0.133506298828125,0.1334863525390625,0.13346640625,0.133446484375,0.1334265625,0.133406640625,0.13338675537109376,0.1333668701171875,0.1333469970703125,0.1333271240234375,0.13330726318359376,0.13328741455078125,0.1332676025390625,0.13324776611328126,0.1332279541015625,0.133208154296875,0.13318837890625,0.133168603515625,0.13314881591796876,0.133129052734375,0.13310931396484374,0.13308955078125,0.1330698486328125,0.1330501220703125,0.1330303955078125,0.13301070556640626,0.1329910400390625,0.13297135009765626,0.1329516845703125,0.13293203125,0.13291239013671874,0.1328927490234375,0.13287314453125,0.1328535400390625,0.132833935546875,0.1328143310546875,0.1327947509765625,0.1327751708984375,0.132755615234375,0.132736083984375,0.1327165283203125,0.1326969970703125,0.13267747802734375,0.132657958984375,0.13263848876953124,0.132618994140625,0.1325994873046875,0.13258004150390626,0.13256058349609376,0.13254112548828126,0.1325217041015625,0.13250228271484374,0.132482861328125,0.1324634521484375,0.1324440673828125,0.132424658203125,0.1324052734375,0.1323859375,0.132366552734375,0.132347216796875,0.13232789306640624,0.1323085693359375,0.13228924560546876,0.1322699462890625,0.1322506591796875,0.13223135986328124,0.1322120849609375,0.132192822265625,0.1321735595703125,0.13215433349609376,0.1321350830078125,0.132115869140625,0.1320966552734375,0.13207742919921875,0.13205823974609374,0.1320390380859375,0.13201988525390626,0.13200074462890626,0.1319815673828125,0.13196240234375,0.13194326171875,0.13192413330078126,0.13190504150390625,0.13188592529296875,0.13186683349609374,0.13184775390625,0.1318286865234375,0.1318095947265625,0.13179053955078124,0.13177149658203124,0.1317524658203125,0.13173343505859375,0.1317144287109375,0.13169539794921875,0.1316763916015625,0.131657421875,0.13163843994140625,0.1316194580078125,0.13160048828125,0.13158154296875,0.13156260986328125,0.1315436767578125,0.131524755859375,0.13150584716796876,0.1314869384765625,0.13146806640625,0.131449169921875,0.13143028564453124,0.1314114013671875,0.1313925537109375,0.13137373046875,0.1313548828125,0.13133607177734374,0.131317236328125,0.13129842529296876,0.131279638671875,0.13126083984375,0.13124208984375,0.131223291015625,0.131204541015625,0.131185791015625,0.131167041015625,0.1311483154296875,0.13112958984375,0.131110888671875,0.13109217529296874,0.1310734619140625,0.13105478515625,0.1310361083984375,0.13101744384765626,0.13099879150390625,0.1309801513671875,0.1309614990234375,0.13094288330078124,0.1309242431640625,0.13090565185546876,0.1308870361328125,0.13086845703125,0.1308498779296875,0.13083128662109375,0.1308127197265625,0.1307941650390625,0.130775634765625,0.13075709228515625,0.13073856201171874,0.13072001953125,0.130701513671875,0.1306830322265625,0.1306645263671875,0.130646044921875,0.13062757568359376,0.1306091064453125,0.130590673828125,0.13057220458984375,0.1305537841796875,0.1305353515625,0.13051693115234375,0.1304985107421875,0.1304801025390625,0.13046173095703126,0.13044334716796874,0.130424951171875,0.130406591796875,0.130388232421875,0.1303698974609375,0.1303515625,0.1303332275390625,0.130314892578125,0.1302966064453125,0.1302782958984375,0.13025999755859374,0.13024169921875,0.13022344970703126,0.13020516357421874,0.1301869140625,0.13016865234375,0.1301504150390625,0.1301322021484375,0.13011396484375,0.130095751953125,0.13007755126953124,0.1300593505859375,0.13004117431640624,0.1300229736328125,0.130004833984375,0.129986669921875,0.129968505859375,0.1299503662109375,0.1299322265625,0.129914111328125,0.1298959716796875,0.1298778564453125,0.129859765625,0.12984168701171875,0.1298236083984375,0.1298054931640625,0.129787451171875,0.12976939697265624,0.1297513671875,0.12973330078125,0.129715283203125,0.1296972412109375,0.129679248046875,0.12966123046875,0.12964326171875,0.12962525634765626,0.129607275390625,0.129589306640625,0.12957132568359375,0.12955338134765626,0.12953543701171874,0.1295175048828125,0.1294995849609375,0.1294816650390625,0.1294637451171875,0.1294458251953125,0.1294279541015625,0.12941005859375,0.1293921875,0.12937431640625,0.1293564453125,0.12933858642578125,0.129320751953125,0.1293029296875,0.1292850830078125,0.12926728515625,0.129249462890625,0.129231640625,0.1292138427734375,0.1291960693359375,0.1291782958984375,0.12916053466796876,0.1291427734375,0.1291250244140625,0.12910726318359375,0.12908955078125,0.1290718017578125,0.1290541015625,0.1290364013671875,0.1290186767578125,0.12900098876953126,0.12898330078125,0.12896563720703125,0.1289479736328125,0.1289302978515625,0.128912646484375,0.1288949951171875,0.12887734375,0.12885972900390624,0.1288421142578125,0.1288244873046875,0.1288069091796875,0.1287892822265625,0.1287717041015625,0.128754150390625,0.12873656005859374,0.1287190185546875,0.1287014404296875,0.12868388671875,0.128666357421875,0.128648828125,0.128631298828125,0.12861378173828125,0.1285962890625,0.1285787841796875,0.12856126708984375,0.128543798828125,0.1285263427734375,0.1285088623046875,0.12849140625,0.1284739501953125,0.1284565185546875,0.1284390869140625,0.1284216552734375,0.1284042236328125,0.12838682861328124,0.1283694091796875,0.12835201416015626,0.12833463134765624,0.1283172607421875,0.12829990234375,0.12828253173828125,0.128265185546875,0.1282478271484375,0.1282304931640625,0.128213134765625,0.1281958251953125,0.128178515625,0.12816121826171875,0.128143896484375,0.128126611328125,0.128109326171875,0.128092041015625,0.12807479248046874,0.12805751953125,0.1280402587890625,0.1280230224609375,0.12800579833984374,0.1279885498046875,0.127971337890625,0.1279541259765625,0.1279369140625,0.12791971435546876,0.12790252685546874,0.12788533935546875,0.1278681640625,0.1278510009765625,0.127833837890625,0.12781668701171875,0.127799560546875,0.1277823974609375,0.12776527099609375,0.12774814453125,0.1277310302734375,0.127713916015625,0.127696826171875,0.127679736328125,0.127662646484375,0.1276455810546875,0.1276284912109375,0.1276114501953125,0.127594384765625,0.1275773681640625,0.127560302734375,0.12754327392578124,0.12752625732421874,0.127509228515625,0.127492236328125,0.1274752197265625,0.1274582275390625,0.12744124755859376,0.127424267578125,0.1274072998046875,0.1273903564453125,0.127373388671875,0.1273564208984375,0.1273395263671875,0.12732255859375,0.1273056396484375,0.127288720703125,0.1272718017578125,0.1272549072265625,0.127238037109375,0.12722113037109375,0.127204248046875,0.1271873779296875,0.1271705078125,0.1271536376953125,0.12713680419921874,0.1271199462890625,0.1271031005859375,0.127086279296875,0.1270694580078125,0.12705263671875,0.12703583984375,0.12701903076171875,0.12700224609375,0.12698546142578124,0.1269686767578125,0.1269519287109375,0.12693515625,0.126918408203125,0.1269016357421875,0.126884912109375,0.1268681640625,0.12685145263671874,0.126834716796875,0.1268179931640625,0.12680130615234375,0.126784619140625,0.126767919921875,0.1267512451171875,0.1267345703125,0.1267178955078125,0.1267012451171875,0.12668458251953124,0.1266679443359375,0.1266512939453125,0.12663465576171876,0.1266180419921875,0.12660142822265624,0.126584814453125,0.126568212890625,0.1265515869140625,0.126535009765625,0.1265184326171875,0.12650184326171876,0.1264852783203125,0.12646873779296874,0.12645218505859376,0.12643564453125,0.126419091796875,0.12640255126953126,0.1263860107421875,0.12636951904296875,0.1263530029296875,0.1263364990234375,0.12631998291015625,0.126303515625,0.12628701171875,0.126270556640625,0.12625406494140626,0.12623760986328125,0.126221142578125,0.1262047119140625,0.12618826904296876,0.126171826171875,0.1261553955078125,0.12613897705078125,0.1261225830078125,0.1261061767578125,0.12608975830078126,0.126073388671875,0.126056982421875,0.1260406005859375,0.1260242431640625,0.126007861328125,0.12599151611328124,0.1259751708984375,0.1259588134765625,0.12594248046875,0.1259261474609375,0.125909814453125,0.1258934814453125,0.125877197265625,0.1258609130859375,0.1258446044921875,0.1258283203125,0.1258120361328125,0.125795751953125,0.12577950439453126,0.12576322021484376,0.12574697265625,0.12573072509765626,0.12571446533203126,0.1256982421875,0.1256820068359375,0.1256657958984375,0.12564957275390626,0.1256333740234375,0.12561717529296876,0.1256009765625,0.1255847900390625,0.125568603515625,0.1255524169921875,0.1255362548828125,0.1255201171875,0.125503955078125,0.12548779296875,0.1254716796875,0.1254555419921875,0.125439404296875,0.1254232666015625,0.12540716552734374,0.1253910400390625,0.12537496337890625,0.1253588623046875,0.1253427734375,0.12532669677734376,0.12531063232421874,0.12529453125,0.1252784912109375,0.12526246337890626,0.12524638671875,0.12523037109375,0.12521431884765624,0.125198291015625,0.125182275390625,0.12516627197265626,0.125150244140625,0.1251342529296875,0.12511824951171874,0.1251022705078125,0.1250863037109375,0.1250703125,0.125054345703125,0.1250384033203125,0.1250224365234375,0.12500645751953124,0.12499052734375,0.1249745849609375,0.1249586669921875,0.1249427490234375,0.1249268310546875,0.1249109375,0.12489503173828125,0.124879150390625,0.12486324462890624,0.12484736328125,0.12483148193359375,0.12481561279296875,0.124799755859375,0.12478388671875,0.12476802978515625,0.124752197265625,0.1247363525390625,0.12472052001953125,0.1247046875,0.1246888671875,0.124673046875,0.1246572509765625,0.124641455078125,0.1246256591796875,0.1246098876953125,0.124594091796875,0.1245783203125,0.124562548828125,0.1245468017578125,0.1245310302734375,0.124515283203125,0.124499560546875,0.12448380126953125,0.12446806640625,0.12445234375,0.1244366455078125,0.12442091064453124,0.124405224609375,0.1243895263671875,0.12437381591796876,0.1243581298828125,0.1243424560546875,0.12432679443359375,0.12431112060546876,0.124295458984375,0.1242798095703125,0.12426416015625,0.1242485107421875,0.12423287353515625,0.1242172607421875,0.12420162353515625,0.1241860107421875,0.12417041015625,0.1241548095703125,0.124139208984375,0.1241236083984375,0.1241080078125,0.124092431640625,0.12407686767578124,0.1240613037109375,0.1240457275390625,0.12403017578125,0.12401463623046875,0.1239990966796875,0.123983544921875,0.1239680419921875,0.1239525146484375,0.1239369873046875,0.123921484375,0.12390596923828125,0.123890478515625,0.1238749755859375,0.12385947265625,0.123843994140625,0.12382852783203124,0.1238130615234375,0.1237975830078125,0.12378214111328124,0.1237666748046875,0.12375125732421875,0.123735791015625,0.12372037353515625,0.1237049560546875,0.1236895263671875,0.12367410888671874,0.12365870361328125,0.1236432861328125,0.1236279052734375,0.1236125244140625,0.1235971435546875,0.12358175048828125,0.1235663818359375,0.123551025390625,0.12353564453125,0.1235203125,0.1235049560546875,0.123489599609375,0.123474267578125,0.123458935546875,0.123443603515625,0.1234283203125,0.12341298828125,0.12339769287109376,0.123382373046875,0.12336708984375,0.12335179443359374,0.1233365234375,0.1233212646484375,0.12330596923828124,0.1232906982421875]};\n",
       "\n",
       "  var data = [data0];\n",
       "  var layout = {\"title\":\"loss by time\"};\n",
       "\n",
       "  Plotly.plot('plot-1074041012', data, layout);\n",
       "})();\n",
       "});\n",
       "      "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres16_1\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"plot-1074041012\"\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plotly.JupyterScala.init()\n",
    "Seq(Scatter(lossSeq.indices, lossSeq)).plot(title = \"loss by time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have learned the follows in this article:\n",
    "\n",
    "* Prepare and process CIFAR10 data\n",
    "* Write softmax classifier\n",
    "* Use the prediction image of the neural network written by softmax classifier to match with the probability of each category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
